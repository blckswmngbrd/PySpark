from pyspark.sql.types import *
from pyspark import SQLContext, Rows
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("BTC")

sc = SparkContext.getOrCreate(conf = conf)

sqlContext = SQLContext(sc)

data_file = '/home/riverstone/Downloads/NW_BTC_Global_Price_INDX'

BTC_raw = sc.textFile(data_file).map(lambda b: b.split(","))

row_data =BTC_raw.map(lambda p: Row(Date=p[0],Open=p[1],High=p[2],Low=p[3],Close=p[4],Volume=p[5],VWAP=p[6],TWAP=p[7]))

BTC_df = BTC_raw.toDF()
BTC_df.show()
BTC_df.printSchema()

BTC_df.describe("Volume").show()T

BTC_df.registerTempTable("coins")

BTC_df.withColumn('%DifVol',(BTC_df.Volume - meanVol)/meanVol).withColumn('Range',BTC_df.High-BTC_df.Low).select('Date','Range','%DifVol').show(5)

sqlContext.sql('''SELECT Date, Close FROM coins WHERE Close > VWAP AND Close > Open AND (High-Low)/Low> 0.25 ''').show()

